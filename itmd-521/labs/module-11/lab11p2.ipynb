{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reading credentials directly\n",
    "creds_file = open(f\"/home/{os.getenv('USER')}/creds.txt\", \"r\").read().strip().split(\",\")\n",
    "accesskey, secretkey = creds_file[0], creds_file[1]\n",
    "\n",
    "db_creds_file = open(f\"/home/{os.getenv('USER')}/database-creds.txt\", \"r\").read().strip().split(\",\")\n",
    "dbusername, dbpassword = db_creds_file[0], db_creds_file[1]\n",
    "\n",
    "# Spark configuration\n",
    "conf = SparkConf()\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.3,mysql:mysql-connector-java:8.0.33')\n",
    "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "conf.set('spark.hadoop.fs.s3a.access.key', accesskey)\n",
    "conf.set('spark.hadoop.fs.s3a.secret.key', secretkey)\n",
    "conf.set('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "conf.set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "conf.set('spark.hadoop.fs.s3a.committer.magic.enabled', 'true')\n",
    "conf.set('spark.hadoop.fs.s3a.committer.name', 'magic')\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"http://system54.rice.iit.edu\")\n",
    "conf.setMaster(\"spark://sm.service.consul:7077\")\n",
    "conf.set(\"spark.driver.memory\", \"16g\")\n",
    "conf.set(\"spark.executor.memory\", \"16g\")\n",
    "conf.set(\"spark.cores.max\", '10')\n",
    "conf.set('spark.executor.cores', '1')\n",
    "\n",
    "# Spark session initialization\n",
    "spark = SparkSession.builder.appName(\"rharidasu_module_11\") \\\n",
    "    .config('spark.driver.host', 'spark-edge.service.consul').config(conf=conf).getOrCreate()\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", \"10000\")\n",
    "\n",
    "\n",
    "# Custom schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"WeatherStation\", StringType(), True),\n",
    "    StructField(\"WBAN\", StringType(), True),\n",
    "    StructField(\"ObservationDate\", DateType(), True),\n",
    "    StructField(\"ObservationHour\", IntegerType(), True),\n",
    "    StructField(\"Latitude\", FloatType(), True),\n",
    "    StructField(\"Longitude\", FloatType(), True),\n",
    "    StructField(\"Elevation\", IntegerType(), True),\n",
    "    StructField(\"WindDirection\", IntegerType(), True),\n",
    "    StructField(\"WDQualityCode\", IntegerType(), True),\n",
    "    StructField(\"SkyCeilingHeight\", IntegerType(), True),\n",
    "    StructField(\"SCQualityCode\", IntegerType(), True),\n",
    "    StructField(\"VisibilityDistance\", IntegerType(), True),\n",
    "    StructField(\"VDQualityCode\", IntegerType(), True),\n",
    "    StructField(\"AirTemperature\", FloatType(), True),\n",
    "    StructField(\"ATQualityCode\", IntegerType(), True),\n",
    "    StructField(\"DewPoint\", FloatType(), True),\n",
    "    StructField(\"DPQualityCode\", IntegerType(), True),\n",
    "    StructField(\"AtmosphericPressure\", FloatType(), True),\n",
    "    StructField(\"APQualityCode\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load data from 60.json (MinIO) and 50 (MariaDB table)\n",
    "json_path = \"s3a://rharidasu/output/60.json\"\n",
    "df_60 = (spark.read.schema(custom_schema)\n",
    "         .option(\"recursiveFileLookup\", \"true\")\n",
    "         .json(json_path)\n",
    "         .select(\"WeatherStation\", \"ObservationDate\", \"AirTemperature\")  # Select required columns\n",
    "         .repartition(4))\n",
    "\n",
    "df_50 = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", 'jdbc:mysql://system75.rice.iit.edu:3306/rharidasu') \\\n",
    "    .option(\"dbtable\", \"(SELECT WeatherStation, ObservationDate, AirTemperature FROM dbrc50 WHERE AirTemperature IS NOT NULL) AS subquery\") \\\n",
    "    .option(\"user\", dbusername) \\\n",
    "    .option(\"password\", dbpassword) \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"fetchsize\", 1000) \\\n",
    "    .option(\"partitionColumn\", \"ObservationDate\") \\\n",
    "    .option(\"lowerBound\", \"1950-01-01\") \\\n",
    "    .option(\"upperBound\", \"1970-12-31\") \\\n",
    "    .option(\"numPartitions\", 10) \\\n",
    "    .load()\n",
    "\n",
    "# Apply filters and create temp tables\n",
    "df_60_filtered = df_60.filter(\n",
    "    (col(\"ObservationDate\").isNotNull()) &\n",
    "    (col(\"AirTemperature\").isNotNull()) &\n",
    "    (year(col(\"ObservationDate\")).between(1950, 1970)) &\n",
    "    (month(col(\"ObservationDate\")) == 2) &\n",
    "    (col(\"AirTemperature\").between(-100, 100))\n",
    ")\n",
    "df_60_filtered.createOrReplaceTempView(\"filtered_60\")\n",
    "\n",
    "df_50_filtered = df_50.filter(\n",
    "    (col(\"ObservationDate\").isNotNull()) &\n",
    "    (col(\"AirTemperature\").isNotNull()) &\n",
    "    (year(col(\"ObservationDate\")).between(1950, 1970)) &\n",
    "    (month(col(\"ObservationDate\")) == 2) &\n",
    "    (col(\"AirTemperature\").between(-100, 100))\n",
    ")\n",
    "df_50_filtered.createOrReplaceTempView(\"filtered_50\")\n",
    "\n",
    "# Combine the filtered data using SQL and create a combined temp table\n",
    "combined_df = spark.sql(\"\"\"\n",
    "    SELECT * FROM filtered_60\n",
    "    UNION ALL\n",
    "    SELECT * FROM filtered_50\n",
    "\"\"\")\n",
    "combined_df.createOrReplaceTempView(\"february_data\")\n",
    "\n",
    "# Task 1: Count the number of records\n",
    "record_count_df = spark.sql(\"SELECT COUNT(*) AS record_count FROM february_data\")\n",
    "\n",
    "# Task 2: Average air temperature for February\n",
    "avg_temp_df = spark.sql(\"SELECT AVG(AirTemperature) AS avg_temp FROM february_data\")\n",
    "\n",
    "# Task 3: Median air temperature for February\n",
    "median_temp_df = spark.sql(\"\"\"\n",
    "    SELECT percentile_approx(AirTemperature, 0.5) AS median_temp\n",
    "    FROM february_data\n",
    "\"\"\")\n",
    "\n",
    "# Task 4: Standard deviation of air temperature for February\n",
    "stddev_temp_df = spark.sql(\"\"\"\n",
    "    SELECT stddev(AirTemperature) AS stddev_temp\n",
    "    FROM february_data\n",
    "\"\"\")\n",
    "\n",
    "# Task 5: Average air temperature per StationID for each year in February\n",
    "avg_temp_per_station_year = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        WeatherStation,\n",
    "        YEAR(ObservationDate) AS Year,\n",
    "        AVG(AirTemperature) AS avg_temp\n",
    "    FROM february_data\n",
    "    GROUP BY WeatherStation, YEAR(ObservationDate)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Output path\n",
    "output_path = \"s3a://rharidasu/module-11\"\n",
    "\n",
    "# Write results to CSV files\n",
    "record_count_df.coalesce(1).write.csv(output_path + \"/record_count\", mode=\"overwrite\", header=True)\n",
    "avg_temp_df.coalesce(1).write.csv(output_path + \"/avg_temp\", mode=\"overwrite\", header=True)\n",
    "median_temp_df.coalesce(1).write.csv(output_path + \"/median_temp\", mode=\"overwrite\", header=True)\n",
    "stddev_temp_df.coalesce(1).write.csv(output_path + \"/stddev_temp\", mode=\"overwrite\", header=True)\n",
    "avg_temp_per_station_year.coalesce(1).write.csv(output_path + \"/avg_temp_per_station_year\", mode=\"overwrite\", header=True)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
